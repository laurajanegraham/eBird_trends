{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extracts for occupancy models\n",
    "This code extracts the species presence/absence and covariatates data from the SQL database and rearranges it into the correct format for the occupancy models. These are then run using R/Jags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sqlalchemy import create_engine\n",
    "from rpy2.robjects import pandas2ri, r\n",
    "pandas2ri.activate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select the required data - at the moment working with a very small subset (hummingbirds, Colorado, 2008-2014) while testing out code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97978, 13)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to database\n",
    "engine = create_engine('postgresql://postgres:password123.@localhost:5432/ebird_us_data')\n",
    "\n",
    "# Extract the data for US, humminbirds, from 2004-2014\n",
    "humdat = pd.read_sql_query(\"\"\"SELECT info.sampling_event_id, \n",
    "                           loc_id, \n",
    "                           latitude, \n",
    "                           longitude, \n",
    "                           year, \n",
    "                           month, \n",
    "                           day, \n",
    "                           time,\n",
    "                           effort_hrs,\n",
    "                           number_observers,\n",
    "                           pop00_sqmi,\n",
    "                           housing_density,\n",
    "                           sppres.species\n",
    "                           FROM ebird_checklist_info info\n",
    "                           INNER JOIN ebird_checklist_species sppres\n",
    "                               ON info.sampling_event_id = sppres.sampling_event_id\n",
    "                           INNER JOIN ebird_species_info spinfo\n",
    "                               ON sppres.species = spinfo.species\n",
    "                           WHERE state_province = 'Colorado' \n",
    "                           AND family = 'Trochilidae' \n",
    "                           AND year >=2008\n",
    "                           AND month IN (5, 6, 7) \n",
    "                           AND count_type IN ('P21', 'P22', 'P34')\"\"\"\n",
    "                           , con=engine)\n",
    "\n",
    "# check the data\n",
    "humdat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Convert the observation date to full date and give a value of 1 to each observation (will be used later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "humdat['obs_date'] = humdat.apply(lambda x: datetime.datetime.strptime(str(x['year']) + ' ' + str(x['day']), '%Y %j').strftime('%Y-%m-%d'), axis = 1)\n",
    "humdat['value'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here I'm reducing the data so that I only have month/location combinations with >= 3 replicates (equal to the lowest threshold used by Kamp et al. 2016) and then taking all locations where at least 5 years have +3 observations. This is subject to change after discussions because the choices are essentially arbitrary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "humdat_sml = humdat[['obs_date', 'year', 'loc_id']].drop_duplicates().groupby(['loc_id', 'year']).size().reset_index()\n",
    "humdat_sml.columns = ['loc_id', 'year', 'obs']\n",
    "humdat_month_location = humdat_sml.pivot(index = 'loc_id', columns = 'year', values = 'obs').fillna(value=0)\n",
    "humdat_location_obs = humdat_month_location.apply(lambda x: (x >= 3).sum(), axis = 1)\n",
    "humdat_location = humdat_location_obs[humdat_location_obs >= 3].reset_index()\n",
    "humdat_obs = humdat[humdat.loc_id.isin(humdat_location.loc_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humdat_location.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38354"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humdat_obs.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the locations before and after data pruning to plot for comparison in r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rpy2.rinterface.NULL"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_sml = humdat_obs[['loc_id', 'latitude', 'longitude']].drop_duplicates()\n",
    "locations_full = humdat[['loc_id', 'latitude', 'longitude']].drop_duplicates()\n",
    "r_locations_sml = pandas2ri.py2ri(locations_sml)\n",
    "r.assign(\"locations_sml\", r_locations_sml)\n",
    "r(\"save(locations_sml, file='D:/eBird_trends/locations_sml.rda')\")\n",
    "r_locations_full = pandas2ri.py2ri(locations_full)\n",
    "r.assign(\"locations_full\", r_locations_full)\n",
    "r(\"save(locations_full, file='D:/eBird_trends/locations_full.rda')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to get the values of the covariates for each of the replicates. Covariates are:\n",
    "\n",
    "- time \n",
    "- effort (hrs)\n",
    "- day\n",
    "- year\n",
    "- population per square mile\n",
    "- housing density (sq mile)\n",
    "- number of observers \n",
    "\n",
    "(NB. replicates are being reduced down to one replicate per day, so we will be taking the earliest time and a sum of the effort and number of observers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "covariate_dat = humdat_obs.groupby(['loc_id', 'obs_date']).agg({'time': np.min, 'effort_hrs': np.sum, 'day': np.mean, 'year': np.mean, 'pop00_sqmi': np.mean, 'housing_density': np.mean, 'number_observers': np.sum}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11107, 9)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariate_dat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get information for input to the function - maximum number of replicates; unique locations, years and species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the maximum number of unique sampling replicates \n",
    "max_rep = humdat_obs[['obs_date', 'year', 'loc_id']].drop_duplicates().sort_values(['loc_id', 'obs_date'])\n",
    "max_rep = max(max_rep.groupby(['loc_id', 'year']).size())\n",
    "year = humdat_obs.year.unique()\n",
    "loc_id = humdat_obs.loc_id.unique()\n",
    "species = pd.DataFrame(humdat_obs.species.unique(), columns = ['species'])\n",
    "species_full = pd.DataFrame(humdat.species.unique(), columns = ['species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Species lost by filtering the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Eugenes_fulgens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           species\n",
       "8  Eugenes_fulgens"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_species = species_full[~species_full.species.isin(species.species)]\n",
    "missing_species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function to get the species presence absence data into the correct order, set up a year x location x species x replicate array for input to the occupancy model and then loop through years and locations to create it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_juggle(in_dat, timestep, location, species, max_rep):\n",
    "    dat_sub = in_dat[(in_dat.year == timestep) & (in_dat.loc_id == location)]\n",
    "    dat_sub = dat_sub[['species', 'obs_date', 'value']].drop_duplicates()\n",
    "    # get lookup for date/time and replicate\n",
    "    if dat_sub.shape[0]==0:\n",
    "        out_dat = np.zeros((species.size, max_rep))\n",
    "        out_dat[:] = np.NaN\n",
    "    else:\n",
    "        sampling_reps = dat_sub[['obs_date']].drop_duplicates().sort_values(['obs_date'])\n",
    "        sampling_reps['replicate'] = range(1, len(sampling_reps) + 1)\n",
    "        \n",
    "        dat_samp_reps = dat_sub.merge(sampling_reps)\n",
    "        dat_wide = dat_samp_reps.pivot(index = 'species',columns = 'replicate', values = 'value').reset_index()\n",
    "        dat_species = species.merge(dat_wide,how = \"left\").fillna(value = 0)\n",
    "        extra_cols = list(range(dat_species.columns[dat_species.shape[1]-1]+1, max_rep+1))\n",
    "        extra_cols = pd.DataFrame(index = dat_species.index, columns = extra_cols)\n",
    "        out_dat = pd.concat([dat_species, extra_cols], axis = 1).drop(['species'], axis = 1).as_matrix()\n",
    "        print( str(timestep) + ' ' + str(location) + ' done...')\n",
    "    return out_dat;\n",
    "\n",
    "# obs data and covariates that are location/replicate specific and need to go through loop\n",
    "wide_dat = np.zeros((year.size, loc_id.size, species.size, max_rep))\n",
    "time = np.zeros((year.size, loc_id.size, max_rep))\n",
    "number_observers = np.zeros((year.size, loc_id.size, max_rep))\n",
    "effort_hrs = np.zeros((year.size, loc_id.size, max_rep))\n",
    "\n",
    "# other covariates\n",
    "day = np.r_[1:max_rep+1] # currently it's set just to the same as the rep number - might need change?\n",
    "covariate_loc = covariate_dat[['loc_id', 'pop00_sqmi', 'housing_density']].drop_duplicates().sort_values(['loc_id'])\n",
    "pop00_sqmi = covariate_loc.pop00_sqmi.values\n",
    "housing_density = covariate_loc.housing_density.values\n",
    "\n",
    "\n",
    "for i in range(0, len(year)):\n",
    "    for j in range(0, len(loc_id)):\n",
    "        wide_dat[i][j] = data_juggle(humdat_obs, year[i], loc_id[j], species, max_rep)\n",
    "        covariate_sub = dat_sub = in_dat[(in_dat.year == year[i]) & (in_dat.loc_id == loc_id[i])]\n",
    "        if covariate_sub.shape[0]==0:\n",
    "            time[i][j] = np.NaN\n",
    "        else:\n",
    "            time[i][j] = covariate_sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
